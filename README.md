# Реализация модуля полнотекстового поиска для СУБД Redis

Содержание - поправить

1. Введение
2. Описание предметной области
   1. Требования к модулю
      1. Функциональные требования
      2. Нефункциональные требования
3. Проектирование
   1. Системная архитектура
   2. Программная архитектура
      1. Хранилище документов
      2. Репликация данных с ведущего узла
      3. Построение инвертированного индекса
      4. Индексирование существующих документов
      5. Выполнение запроса
      6. Ранжирование результатов
      7. Парсинг запросов
      8. TCP-сервер с использованием протокола RESP
4. Реализация
   1. Стек технологий
   2. Разработка
      1. Реализация индексной реплики
      2. Реализация встраиваемого модуля
5. Тестирование


## Введение

**TODO: написать нормальное введение с четкой целью работы**

## Описание предметной области

Amazon Web Services - это самая распространенная в мире облачная платформа, 
предлагающая большое количество сервисов, упрощающие клиентам создание информационных систем.
Двумя продуктами, которые будут рассмотрены далее, являются Amazon ElastiCache для Redis и Amazon MemoryDB для Redis.

Amazon ElastiCache для Redis - это сервис, предоставляющий полностью управляемую СУБД Redis.
Redis является быстрой нереляционной системой хранения данных в формате "ключ-значение" и используется в основном для кэширования часто используемых данных.
Redis является редизентной СУБД, т.е. хранит данные в памяти, благодаря чему обеспечивается очень большая скорость операций как чтения, так и записи - в среднем менее миллисекунды.

Redis поддерживает репликацию данных между единственным лидером, который принимает операции записи, 
и несколькими репликами - вторичными узлами. Реплики принимают только запросы на чтение, что позволяет обрабатывать больше таких операций.
Также, репликация повышает надежность СУБД, т.к. при внезапной остановке или временной недоступности лидера один из вторичных узлов
берет на себя его роль, благодаря этому СУБД доступна на запись. 
При этом Redis не предоставляет гарантий сохранности данных, т.к. это потребовало бы значительного снижения производительности.
Репликация происходит асинхронно, т.е. операция чтения с реплики может не содержать данных, 
которые были записаны на лидер, но еще не были реплицированы.

Благодаря простой структуре хранения данных "ключ-значение", Redis поддерживает кластеризацию, разделяя данные на несколько шардов. 
Данные разделяются на 16384 хэш-слота, каждому шарду присваивается набор хэш-слотов. 
Для того, чтобы определить к какому хэш-слоту, а следовательно, и к какому шарду относятся данные, необходимо вычислить значение хэш-функции ключа и найти остаток от деления на количество шардов в кластере.

Архитектура кластера СУБД Redis представлена на следующей диаграмме:

![Redis cluster](img/redis_cluster.svg)

```plantuml:redis_cluster
@startuml Redis Cluster

' or component instead of frame maybe...
frame "slots 0-5461" as shard1 {
    node Master as s1m
    node Replica as s1r1
    node Replica as s1r2
    node Replica as s1r3

    port write as w1
    w1 --> s1m

    s1m --> s1r1
    s1m --> s1r2
    s1m --> s1r3

    port read as r1
    r1 ..> s1r1
    r1 ..> s1r2
    r1 ..> s1r3
}

frame "slots 5462-10922" as shard2 {
    node Master as s2m
    node Replica as s2r1
    node Replica as s2r2
    node Replica as s2r3

    port write as w2
    w2 --> s2m

    s2m --> s2r1
    s2m --> s2r2
    s2m --> s2r3
}

frame "slots 10923-16383" as shard3 {
    node Master as s3m
    node Replica as s3r1
    node Replica as s3r2
    node Replica as s3r3

    port write as w3
    w3 --> s3m

    s3m --> s3r1
    s3m --> s3r2
    s3m --> s3r3
}

' s1m <..right..> s2m
' s2m <..right..> s3m
' s1m <..right..> s3m

@enduml
```

Amazon ElastiCache полностью управляет СУБД, обеспечивая мониторинг состояния узлов, 
автоматическое восстановление проблемных узлов, горизонтальное масштабирование с помощью увеличения количества шардов в кластере, 
вертикальное масштабирование с помощью изменения типа виртуальных машин, на которых работают узлы СУБД, 
а также автоматизированное обновление ОС и самой СУБД.

Amazon MemoryDB - это совместимая с Redis СУБД, модифицированная для того, 
чтобы обеспечить надежную персистентность хранения данных с помощью использования распределенного лога транзакций, 
при этом сохраняя преимущества скорости СУБД Redis - чтение менее, чем за 1 мс, и запись менее, чем за 10 мс. 
Гарантия персистентности позволяет использовать MemoryDB как основную СУБД, а не только как систему кэширования данных.

MemoryDB, как и Redis, поддерживает шардирование кластера.

Персистентность обеспечивается с помощью того, что команды записи не реплицируются напрямую на реплики, а сначала записываются в распределенный лог транзакций (называемый Journal - журналом), использующий алгоритм распределенного консенсуса для обеспечения гарантии консистентности. До того момента, как транзакция записана в лог, новые данные недоступны для чтения.

![MemoryDB shard](img/memorydb_shard.svg)

```plantuml:memorydb_shard
@startuml MemoryDB shard

' skinparam linetype ortho

actor Client as c

node Master as m {
    component Redis as mr
    component "Journal Agent" as mja
    mr -> mja
}

database Journal as j

node Replica as r1 {
    component Redis as r1r
    component "Journal Agent" as r1ja
    r1ja -> r1r
}

node Replica as r2 {
    component Redis as r2r
    component "Journal Agent" as r2ja
    r2ja -> r2r
}

c -down-> mr : writes
mja -down-> j : speculative writes

j --> r1ja : commited writes
j --> r2ja : commited writes

@enduml
```

Redis позволяет сохранять в качестве значений не только строки и числа, 
а также такие структуры данных, как списки, т.н. хэши - объекты, представляющие из себя хэш-таблицы, JSON-документы, и т.д.

### Требования к модулю полнотекстового поиска

Несмотря на то, что основное предназначение систем типа "ключ-значение" - быстро находить значение по заданному ключу,
у пользователей в некоторых случаях появляется потребность обеспечить поиск по сохраненным значениям. 
В частности, достаточно часто появляется необходимость искать объекты по значению поля хэша или JSON-документа.
Одним из самых востребованных является полнотекстовый поиск, необходимый для того, 
чтобы клиенты могли очень быстро предоставлять своим пользователям результаты их поисковых запросов.
Возможность поиска становится еще более необходимой для Amazon MemoryDB, 
который используется многими клиентами как основное хранилище данных.

Данный функционал реализован в модуле RediSearch. 
Он обеспечивает построение индексов и полнотекстового поиска с поддержкой логичских операторов над подзапросами, запросов на основе префикса слова, а также некоторых других возможностей.

Несмотря на широкие возможности данного модуля, 
Amazon Web Services не может использовать его для своих продуктов Amazon ElastiCache и Amazon MemoryDB,
т.к. он использует лицензию Redis Source Available License 2.0, 
которая запрещает предоставление этого модуля пользователям с бизнес-моделью "ПО как сервис", 
т.е. той моделью, по которой работает AWS.

В связи с этим появилась необходимость создать модуль полнотекстового поиска, 
реализующий основные возможности RediSearch. 

### Функциональные требования

Для первой версии было принято решение реализовать только основные возможности RediSearch: 
1. Поиск документов, содержащих определенные термины, с поддержкой разных форм слова
2. Поддержка логических операторов между подзапросами
   - объединение данных подзапросов (логический оператор OR) - вернуть все документы, удовлетворяющие одному из подзапросов
   - пересечение данных подзапросов (логический оператор AND) - вернуть все документы, удовлетворяющие всем подзапросам
3. Запрос по значению конкретного поля документа
4. Ранжирование результатов
5. Ограничение максимального количества подходящих результатов

### Нефункциональные требования

Поскольку клиенты выбирают ElastiCache и MemoryDB за низкую задержку ответов и обеспечение выполнения большого количества транзакций в секунду, это требуется и для модуля полнотекстового поиска.
Время отклика для данной системы является важнее, чем точность и полнота результата.

Также, модуль должен обеспечивать не только большую скорость операций поиска, но и достаточную скорость операций записи, чтобы справляться с большим объемом поступающих от клиента данных.

Модуль должен поддерживать как горизонтальное, так и вертикальное масштабирование, т.е. обеспечивать эффективную работу как в условиях 1 ядра процессора и небольшого объема оперативной памяти, так и в условиях большого количества ядер и большого объема оперативной памяти (до 96 ядер и 600 Гб оперативной памяти).

**TODO: написать нормальные численные метрики**

## Проектирование

### Системная архитектура

При проектировании данного модуля возникло два варианта: реализовать модуль для каждого из узлов БД, или реализовать его как отдельно работающий узел кластера. 

Реализация модуля на уровне каждого узла кластера имеет следующие преимущества:
1. Простое распределение нагрузки между узлами, т.к. каждый из узлов может обрабатывать запросы поиска
2. Кластер остается гомогенным, т.е. нет необходимости разделять узлы, которые могут обрабатывать запросы на чтение по ключу и узлы, обрабатывающие запросы полнотекстового поиска

При этом такой тип реализации имеет следующие недостатки:
1. Возрастает нагрузка на каждый узел СУБД, 
  т.к. при сохранении нового объекта требуется произвести обновление индексов, следовательно, узел может обрабатывать меньшую пиковую нагрузку.
2. Обновление происходит не только на ведущем узле, но и на всех репликах, что может повлечь за собой увеличение 
  отставание узлов по репликации данных, 
  что, в свою очередь, может повлечь за собой потерю данных при внезапной остановке ведущего узла
  в случае ElastiCache (не имеющего распределенного лога транзакций), 
  либо более долгое переключение на другой узел в случае MemoryDB (т.к. большее количество данных будет необходимо восстановить из распределенного лога перед тем, как узел сможет претендовать на роль ведущего)
3. Полнотекстовый индекс может занимать достаточно большое количество памяти, 
  что уменьшает максимально возможный объем хранимых данных. 
  Это может оказаться большой проблемой для клиентов, использующих узлы с 0.5 и 1.3 Гб оперативной памяти.
  Это также увеличивает шансы клиента использовать оперативную память узла более, чем на 50%, 
  что не позволяет работать самому эффективному способу создания резервной копии СУБД - созданию копии (fork) процесса с последующей записью резервной копии на диск.
  Использование более медленного способа резервного копирования может полвечь за собой потерю клиентом части данных.

Реализация модуля поиска как отдельного узла также имеет свои преимущества:
1. Нагрузка на выполнение поисковых запросов может масштабироваться независимо, 
  т.е. при необходимоси можно увеличивать максимальное количество запросов полнотекстового поиска с помощью увеличения количества соответствующих индексных узлов
2. Индексные узлы могут быть созданы по требованию, т.е. не требуют модификаций существующих узлов кластера
3. Такая реализация не будет ограничена однопоточным ядром выполнения Redis, 
  что может значительно положительно повлиять на производительность модуля. 
  Однопоточная архитектура Redis весьма эффективна для быстро выполняющихся команд чтения по ключу,
  но может быть неэффективна для более долгих команд поиска.
4. Индексные узлы не ограничены API СУБД Redis для встраиваемых модулей и могут использовать свои оптимизации хранения, создания резервный копий и выполнения зарпосов.

Из недостатков можно отметить:
1. Гетерогенность кластера, что ведет к усложнению управления им, 
  но этот недостаток нивелируется тем, что ElastiCache и MemoryDB является полностью управляемыми кластерами,
  т.е. усложнение управления кластером скрыто от клиента
2. Отсутствие возможности у других узлов обработать запрос - что может быть решено либо проксированием запроса на индексный узел, либо созданием выделенного узла, проксирующего запросы на нужный узел, что уже реализовано в AWS.
3. Большее отставание данных в сравнении с обычными репликами, что является допустимым, но нежелательным.

Для данного проекта было принято решение реализовывать полнотекстовый поиск с помощью отдельных узлов 
с возможностью в будущем интегрировать его в сами узлы СУБД.

![Full text search deployment](img/full_text_search_deployment.svg)

```plantuml:full_text_search_deployment
node "Proxy node" {
    component Proxy as p
}

node Master {
    component Redis as mredis {
      artifact "FTSearch module" as ftsmm
    }
}

node Replica as r1 {
    component Redis as r1redis {
      artifact "FTSearch module" as ftsmr1
    }
}

node Replica as r2 {
    component Redis as r2redis {
      artifact "FTSearch module" as ftsmr2
    }
}

node FTSReplica1 {
    component FTSIndex as ftsi1
}
note left: Full text search\n index

node FTSReplica2 {
    component FTSIndex as ftsi2
}

mredis --> r1redis: replication
mredis --> r2redis
mredis --> ftsi1

p --> mredis: write
p ---> r1redis: read
p ----> ftsi1: search
p ----> ftsi2: search
```
*Архитектура шарда СУБД*

При этом команды создания индекса обрабатываются отдельным модулем, встраиваемым в саму СУБД Redis
в формате статической библиотеки, благодаря чему информация о созданных индексах 
(параметры индекса, но не сам индекс) реплицируется
на все остальные узлы и может быть получена узлом полнотекстового поиска в том же формате,
что и репликация остальных данных, что увеличивает надежность. Также, такая архитектура
позволяет легко реализовать горизонтальное масштабирование запросов поиска по полнотекстовому индексу
с помощью добавления в кластер новых индексирующих узлов, при этом каждый узел независимо от других
получит информацию о созданных индексах от ведущего узла.

Помимо расширения СУБД Redis новой командой создания индекса задачей встраиваемого модуля
является реализация нового типа данных, хранящего информацию об индексе - название, 
список префиксов индексируемых документов и их схема (список индексируемых полей).

### Программная архитектура

Были выиделены следующие основные компоненты модуля:
1. Компонент хранилища данных, по сути являющийся хэш-таблицей, в которой ключу сопоставляется конкретный документ
2. Компонент репликации данных с ведущего узла
3. Компонент индексации документов и хранения инвертированного индекса
4. Компонент лениво вычисляемых итераторов по данных индекса, обеспечивающих выполнение запросов поиска с использованием различных итераторов
5. Компонент парсинга поисковых запросов
6. Серверный компонент, обрабатывающий пользовательские запросы в формате передачи данных RESP, используемом в СУБД Redis.

![Module components](img/module_components.svg)

```plantuml:module_components
class Server

class "RESP Parser" as rp

class "Query parser" as qp

class "Search Engine" as se

class "Index" as i

class "Storage" as s

class "Document" as d

interface "TermIterator" as iter

class "Replication" as repl

Server *-- rp
Server *-- qp
Server *-- se
se o-- i
i -- iter
iter ..> s
s o-- d
repl --> s
```

#### Хранилище документов

Основные функции, которые должно выполнять хранилище документов - это чтение по ключу, 
сохранение нового документа, удаление документа и переименование ключа. 
Функция обновления документа может быть реализована через удаление существующей версии и 
сохранение обновленной версии как нового документа. 
Такой подход связан с особенностями инвертированного индекса, которые будут рассмотрены далее.

Для реагирования других компонентов на операции обновления данных (сохранение и удаление) 
хранилище должно также использовать callback-функции, вызываемые после выполнения тех или иных операций.
Для данного модуля требуется реагирование на операцию сохранения для того, 
чтобы проиндексировать новый документ, а также на операцию удаления, для того, чтобы удалить данные из индекса.

Также, поскольку новые индексы могут быть созданы после того, как часть документов была сохранена,
хранилище должно предоставлять способ пройти по всем сохраненным парам ключ-документ, 
удовлетворяющим определенному префиксу ключа, для их индексации.

В качестве структуры данных для хранилища документов наилучшим образом подходит хэш-таблица,
в которой ключу сопоставляется сам документ. Необходимо также учесть, что доступ к хранилищу
может производиться из нескольких потоков в процессе обработки пользовательских запросов поиска,
что требует синхронизовать доступ к хэш-таблице, 
для чего наиболее подходящим примитивом синхронизации является RW-lock - блокировка чтения-записи,
позволяющая одновременный доступ на чтение нескольким конкурентным задачам, при этом допуская лишь
эксклюзивный доступ только одной задачи изменения данных. Одним из следствий данной архитектуры
является отсутствие преимуществ применения конкурентной репликации данных, 
но обеспечение конкурентного выполнения нескольких запросов полнотекстового поиска.

#### Репликация данных с ведущего узла

В СУБД Redis используется два подхода к сохранению данных на диск: 
сохранение снимков БД в формате RDB и запись самих команд обновления данных в формате AOF - append only file,
т.е. файла, данные в который только добавляются в конец, но не обновляются.
RDB снимок требует меньше дискового пространства, 
т.к. использует бинарный формат и хранит только актуальные данные,
но для эффективного создания такого снимка Redis создает fork (копию) процесса ОС, 
который затем записывает данные из оперативной памяти на диск в этом формате.
Для этого требуется, чтобы как минимум равный текущему размеру БД объем оперативной памяти был свободен,
т.к. все данные хранятся в ней и создается полная копия процесса.

Репликация в СУБД Redis реализована с помощью комбинации этих подходов.
Когда новая реплика устанавливает TCP-подключение к ведущему узлу, она запрашивает сохраненные данные,
которые поступают в формате RDB. В актуальных версиях Redis может отправлять снимок, 
напрямую копируя из оперативной памяти в буфер TCP-подключения, без промежуточного создания снимка на диске.
Все последующие данные реплика получает в формате AOF, т.е. по сути в таком же формате, 
в котором пользователь выполняет команды обновления.

Для отслеживания состояния репликации каждый узел отслеживает количество байт, обработанный 
на текущий момент. Это число называется offset - смещение. Изначально оно является равным
полученному размеру бинарных данных в формате RDB, затем увеличивается каждый раз при получении
команды в формате AOF на ее длину байт. Реплика периодически сообщает текущее значение смещения
на ведущий узел, благодаря чему он может отслеживать состояние каждой из реплик.

В случае отсутствия пользовательских команд реплика получает от ведущего узла команду `ping` в том же AOF формате раз в несколько секунд,
благодаря чему может быть осуществлена проверка того, что ведущий узел продолжает репликацию в нормальном режиме.
Реплика, в свою очередь, должна раз в секунду отправлять по тому же TCP-подключению команду `replconf ack <offset>`, где offset - текущее обработанное смещение.
В случае отсутствия такого ответа от реплики в течение 60 секунд ведущий узел разрывает соединение с ней.

Созданный модуль должен работать с обоими форматами для того, 
чтобы выступать в роли реплики для ведущего узла и не требовать модификаций логики репликации в самой СУБД.
Также, модуль должен правильно работать с командами репликации между ведущим узлом и репликой (отправляя `replconf ack`).

Также, был принят во внимание тот факт, что информация о текущем обработанном смещении реплики 
используется для того, чтобы производить смену ролей (failover) по команде пользователя.
Ведущий узел выбирает для нового лидера ту реплику, смещение которой равно его собственному смещению,
что означает, что реплика полностью синхронизирована и содержит все данные, которые содержит ведущий узел.
Т.к. архитектура модуля не предполагает использование его в качестве ведущего узла,
было принято решение всегда подтверждать смещение на 1 меньше, чем актуальное смещение ведущего узла.

Для унификации обработки двух различных видов репликации было принято решение преобразовывать как
данные из формата RDB, так и команды в формате AOF в объекты классов, реализующих интерфейс Command.
При этом каждый такой класс представляет собой информацию о соответствующей ему конкретной пользовательской
командой, которая может быть получена в текстовом формате с помощью AOF репликации.

![Commands](img/commands.svg)

```plantuml:commands
interface exec.Command {
  Name()
  exec(storage.Storage, search.Engine)
}

class exec.SetCmd {
  Key: string
}

class exec.DelCmd {
  Key: string
}

class exec.HSetCmd {
  Key: string
  Args: HSetArg[]
}

class exec.HDelCmd {
  Key: string
  Fields: string[]
}

class exec.FtCreateCmd {
  Name: string
	Prefixes: string[]
	Fields: string[]
}

class storage.Storage
class search.Engine

exec.Command <|-- exec.SetCmd
exec.Command <|-- exec.DelCmd
exec.Command <|-- exec.HSetCmd
exec.Command <|-- exec.HDelCmd
exec.Command <|-- exec.FtCreateCmd

exec.SetCmd -- storage.Storage
exec.DelCmd -- storage.Storage
exec.HSetCmd -- storage.Storage
exec.HDelCmd -- storage.Storage
exec.FtCreateCmd -- search.Engine
```

*Диаграмма классов команд*

#### Построение инвертированного индекса

Для того, чтобы обрабатывать запросы полнотекстового поиска, необходимо построить инвертированный индекс,
т.е. структуру данных, в котоорой каждому термину, который встретился в документах, сопоставляются все документы, в которых он встретился. Для того, чтобы учитывать взаимное расположение терминов относительно друг друга, индекс также должен включать информацию о позиции термина в документе.

Для того, чтобы индекс позволял осуществлять поиск по различным формам термина, необходимо:
1. токенизировать документ, т.е. разделить на отдельные слова;
2. удалить стоп-слова;
3. запомнить позицию каждого токена, т.е. поле документа, в котором он находится, и его порядковый номер;
4. произвести стемминг или лемматизацию для нахождения основы слова и дальнейшего поиска по ней.

![Full text search process](img/full_text_search_process.svg)

```plantuml:full_text_search_process
@startuml Full text search
allowmixing

file d [
    Document
    ---
    @f1:Hello World!
    @f2:Some text
]

component "Replication" as repl

component "Storage" as s

queue "Processing queue" as pq

component "Tokenizer" as t

component "Token filters" as tf
note bottom : stop words, porterstemmer

component "Inverted index" as i

repl --> s
s -> d
repl -> pq
pq ..> d
pq -> t
t -> tf
tf -> i

@enduml
```

Для данной реализации было принято решение применить стемминг, т.к. он хоть и дает менее точные результаты,
чем лемматизация, но при этом является сильно более производительным, 
что является критически важным для используемой СУБД.

В качестве стандартной реализации был применен алгоритм Стеммер Портера, показывающий хорошие результаты для английского языка, 
при этом в будущем система должна позволять клиенту определять используемый алгоритм для поддержки других языков.

Поскольку удаление из индекса не может быть выполнено моментально,
был применен подход "сборщика мусора", при котором
документ лишь помечается удаленным и впоследствии фильтруется при чтении данных из индекса, 
после чего добавляется в очередь для удаления, что позволяет обработать большую нагрузку,
откладывая обновление индекса.

Обновление документа также может быть осуществлено через удаление предыдущей версии и сохранение новой.

![Document indexing](img/document_indexing.svg)

```plantuml:document_indexing
@startuml

map Hash {
    @field1 => Hello world
    @field2 => 2.0
    @field3 => tag1
}

map "Hash" as HashV2 {
    @field1 => Hi everyone
    @field2 => 2.0
    @field3 => tag1
}

object Document {
    key: string
    hash
    deleted: true
}

Document::hash -> Hash

object "Document V2" as DocumentV2 {
    key
    hash
    deleted: false
}

DocumentV2::hash -> HashV2

map Storage {
    key *--> DocumentV2
}

map docIndex {
    "hello" *--> Document
    "world" *--> Document
    "hi" *--> DocumentV2
    "everyone" *--> DocumentV2
}

object DeleteQueue
DeleteQueue ---> Document

object GC
GC -left-> DeleteQueue

GC --> docIndex

@enduml
```

Оптимальной структурой данных для хранения информации о документах, содержащих конкретный токен,
было выбрано префиксное дерево, т.к. оно позволяет легко в будущем реализовать поиск по префиксу,
доступный в модуле RediSearch, при этом асимптотика поиска по ключу для него O(k), где k - длина ключа.

Для построения инвертированного индекса каждому найденному токену сопоставляется список документов,
в которых он содержится, вместе с информацией о том, в каких полях этого документа он был найден.
Для каждого документа также содержится список вхождений в конкретном поле.

![FTS index classes](img/fts_index_classes.svg)

```plantuml:fts_index_classes
class FTSIndex {
  Fields: string[]
  trie: Trie<string, DocTermOccurrence>
}

FTSIndex o-- DocTermOccurrence

class DocTermOccurrence {
  Doc: *Document
  Fields: BitSet
  Occurrences: FieldTermOccurrence[]
}

DocTermOccurrence o-- FieldTermOccurrence

class FieldTermOccurrence {
  FieldIdx: int
  Offset : int
	Len: int
	Pos: int
}
```

*Архитектура хранения информации о вхождениях токенов в документе*

![FTS index classes example](img/fts_index_classes_example.svg)

```plantuml:fts_index_classes_example
map document {
  title => Hello World!
  description => Hello, such beautiful place as Earth
}

object "DocTermOccurrence: Hello" as helloOcc {
  Doc
  Fields: [title, description]
  Occurrences
}

object "FieldTermOccurrence: Hello in title" as helloTitleOcc {
  fieldIdx: 0
  Offset: 0
  Len: 5
  Pos: 0
}

object "FieldTermOcurrence: Hello in description" as helloDescriptionOcc {
  fieldIdx: 1
  Offset: 0
  Len: 5
  Pos: 0
}

helloOcc::Occurrences --> helloTitleOcc
helloOcc::Occurrences --> helloDescriptionOcc
helloOcc::Doc ...> document

object "DocTermOccurrence: World" as worldOcc {
  Fields: [title]
  Occurrences
}

object "FieldTermOccurrence: World in title" as worldTitleOcc {
  fieldIdx: 0
  Offset: 0
  Len: 5
  Pos: 0
}

worldOcc::Occurrences --> worldTitleOcc
worldOcc::Doc ...> document

map trie {
  hello *--> helloOcc
  world *--> worldOcc
  ... => 
}
```

*Пример хранения информации о вхождениях токенов в документе*

#### Индексирование существующих документов

Индекс может быть создан уже после того, как документы будут сохранены в базе данных,
что требует поддержки их индексирования. В различных СУБД эта задача решается двумя способами:
индексирование документов в момент создания индекса (синхронно) и индексирование документов после его создания (асинхронно). 
В первом случае пользователь может пользоваться индексом сразу после его создания, 
т.к. он уже будет содержать всю необходимую информацию, в отличие от второго случая,
когда некоторое время после создания индекс будет лишь обрабатывать уже существующие данные.
При этом у первого случая есть критический недостаток, из-за которого было принято решение использовать
асинхронную реализацию - для него требуется блокирование клиентского подключения до момента завершения
индексирования, что может занять от нескольких секунд до нескольких часов, из-за чего возрастает риск
потери соединения, после чего либо требуется продолжать индексирование (т.е. переходить в асинхронный режим),
либо отменять создание индекса (т.е. вычислительные ресурсы были потрачены впустую).

Для асинхронного режима создания индекса требуется также отслеживать документы, 
добавленные после создания индекса, но до завершения индексирования существующих документов.
Для этого индекс может содержать очередь документов, которая будет использоваться как временное хранилище документов, требующих обработки. После того, как обработка документов, добавленных до создания индекса, будет закончена, индекс должен обработать все документы, находящиеся в очереди, а затем перейти в нормальный режим работы.

#### Выполнение запроса

Выполнение запроса состоит из применения нескольких операций над данными, сохраненными в инвертированном индексе:
- чтение всех документов, содержащих конкретный термин
- объединение результатов - например, запрос hello | world должен вернуть все документы, содержащие либо термин *hello*, либо термин *world*
- пересечение результатов - например, запрос hello world должен вернуть все документы, содержащие термины *hello* и *world*, при этом отдавая преимущество тем документам, где эти термины находятся ближе друг к другу
- фильтрация результатов по конкретному полю - клиент может указать, что поиск должен выполняться по содержимому конкретного поля документа, например, запрос @title:(hello world) должен вернуть документы, содержащие термины *hello* и *world* в поле документа *title*
- поиск терминов, идущих строго друг за другом - например, запрос hello NEAR world должен вернуть все документы, содержащие термины hello world без любых терминов между ними
- фильтрация определенного количества подходящих результатов - т.к. запросу может удовлетворять большое количество документов, у клиента должна быть возможность указать максимальное количество документов, которые он хочет получить.

Для реализации этих требований было принято решение использовать лениво вычисляемые итераторы.

![Search iterators](img/search_iterators.svg)

```plantuml:search_iterators
interface "TermIterator" as iter {
  Next()
}

class "ReadIterator" as ri
class "UnionIterator" as ui
class "IntersectIterator" as ii
class "FieldIterator" as fi
class "NearIterator" as ni
class "TopNIterator" as ti

iter <|-- ri
iter <|-- ui
iter <|-- ii
iter <|-- fi
iter <|-- ni
iter <|-- ti
```

*Архитектура итераторов*

Пользовательский запрос должен быть превращен в комбинацию этих итераторов, данные из корневого итератора должны быть отправлены клиенту.
Итераторы в листьях этого дерева читают данные из инвертированного индекса, промежуточные итераторы комбинируют полученные данные и производят дополнительную фильтрацию.

![Search iterators example](img/search_iterators_example.svg)

```plantuml:search_iterators_example
object "ReadIterator" as hello {
  hello
}

object "ReadIterator" as world {
  world
}

object "ReadIterator" as sunshine {
  sunshine
}

object "UnionIterator" as union

object "IntersectIterator" as intersect

object "TopNIterator" as topn {
  10
}

union --> world
union --> sunshine
intersect --> union
intersect --> hello
topn --> intersect
```

*Пример итераторов для выполнения запроса `hello (world | sunshine) LIMIT 0 10`*

#### Ранжирование результатов

Важной задачей полнотекстового поиска является не только поиск документов,
содержащих определенные термины, но также и ранжирование результатов.
Одной из самых популярных статистических мер для этого является TF-IDF.

TF (term frequency - частота слова) - отношение количества вхождений термина к общему количеству токенов в документе. Эта метрика показывает важность термина в рамках конкретного документа - чем выше это отношение,
тем больше этот термин встречается в этом документе.

IDF (inverse document frequency - обратная частота документа) - отношение общего числа документов к
числу документов, содержащих определенный термин, т.е. инверсия частоты документов, содержащих термин.
Эта метрика уменьшает вес терминов, которые содержатся в большом количестве документов,
и, следовательно, обладающих низкой селективностью.

Например, допустим, что в базе данных находятся 3 документа.
Каждый состоит из 10 слов. 
Первый документ содержит два вхождения токена hello и одно вхождение токена world.
Второй документ содержит одно вхождение токена hello.
Осуществляется поиск документов, содержащих либо токен hello, либо токен world.

TF токена hello для документв 1 равен 2/10, для документа 2 - 1/10.
IDF токена hello равен log2(1 + 3/2) ~ 1.32, где 3 - общее количество документов, 2 - количество документов, содержащих токен hello.
TF-IDF токена hello для документа 1 равен 2/10 * 1.32 ~ 0.264, для документа 2 - 1/10 * 1.32 ~ 0.132.

TF токена world для документа 1 равен 1/10.
IDF токена world равен log2(1 + 3/1) = 2.
TF-IDF токена world для документа 1 равен 1/10 * 2 ~ 0.2.

Таким образом, вес одного вхождения термина world в данном случае является практически настолько же высоким, 
как два вхождения термина hello.

Для того, чтобы оптимизировать выполнение запроса, TF можно вычислять на этапе индексирования документа,
т.к. если рассматривать документы как неизменяемые сущности, то количество вхождений токена и общее количество токенов известно в этот момент. Для вычисления IDF требуется общее количество документов,
которое неизвестно на момент индексирования документа, 
но DF - количество документов, содержащих определенный токен - также может быть вычислено в момент 
индексирования документа.

После применения данных оптимизаций, при выполнении поиска для вычисления веса определенного документа
требуется лишь вычислить IDF - log2(1 + N/DF), где N - общее количество документов, затем умножив его
на сохраненное значение TF для конкретного документа.

Терминальный итератор TopNIterator, осуществляющий выборку определенного количества вхождений,
производит полное чтение всех найденных вхождений, затем сортирует их по значению TF-IDF, 
после чего производит выборку элементов по количеству.

#### Парсинг запросов

Основные команды, которые необходимо было обрабатывать модулю - это команда создания индекса и 
команда поиска по нему.

Команда создания индекса в ее упрощенном для первой версии модуля виде достаточно проста:
```
FT.CREATE <index> 
  [PREFIX <prefix count> <prefix1> <prefix2> ...]
  SCHEMA <field name 1> TEXT | TAG | NUMERIC ...
```

Ключевые слова регистронезависимы, т.е. могут быть указаны как `ft.create`.

Аргумент PREFIX позволяет указать префиксы ключей для ограничения набора индексируемых документов.
По умолчанию значение префикса **\***, т.е. индексируются все документы.

Аргумент SCHEMA обязателен и содержит список полей документов, которые должны быть проиндексированы,
с указанием типа поля. На данный момент поддерживается только тип TEXT.

Например, создание индекса wiki-docs с префиксом `wiki:` и индексируемыми полями `title` и `content`
будет выглядеть так:
```
FT.CREATE wiki-docs PREFIX 1 SCHEMA title TEXT content TEXT
```

Синтаксис запросов более сложен для парсинга, 
т.к. даже в минимально необходимой для первой версии форме имеет такую структуру:
```
FT.SEARCH <index> <query> [LIMIT <offset> <num>]
```
При этом сам запрос может состоять из нескольких частей, разделенных пробелом 
(что является аналогом логического оператора **И**, 
т.е. будут возвращены документы, удовлетворяющие каждой из чатей).
Каждая часть может быть 
- отдельным словом, которое (возможно, в измененной форме) должно содержаться в документе;
- несколькими словами в кавычках, например, "hello world", что означает, что слова должны идти подряд;
- двумя частями, разделенными оператором **ИЛИ** - вертикальной чертой `|`, например, hello | world;
- подзапросом, состоящим из одной или нескольких частей, сгруппированных с помощью круглых скобок `()` для изменения порядка применения операторов;
- поиском по конкретному полю: `@fieldName:query`, где в качестве query выступает часть запроса, т.е. может быть обрамлена в круглые скобки и использовать логические операторы, например, `@title:(hello | world)`;

#### TCP-сервер с использованием протокола RESP

СУБД Redis использует для клиентских подключений стандартный TCP-сокет,
благодаря чему очень легко создать клиентскую библиотеку для большинства языков программирования.
В качестве протокола коммуникации используется RESP - Redis serialization protocol.
Его преимуществами являются простота, скорость парсинга и человекочитаемость.

В RESP поддерживаются 5 типов данных:
- строки;
- ошибки (по своей сути тоже строки);
- числа;
- длинные строки ("bulk strings"), которые могут содержать до 512 Мб данных;
- массивы.

Тип данных кодируется первым байтом сообщения. 
Так, для строк первым байтом должен быть **+**, для ошибок - **-**, для чисел - **:**, для длинных строк - **$**, для массивов - **\***.

Разные части сообщения в RESP всегда заканчиваются символами конца строки CRLF.

Клиент посылает запросы в формате массива, состоящего из длинных строк. 
Таким образом, клиент может отправить несколько команд в одном запросе, что позволяет значительно увеличить производительность.

Данный протокол используется в нескольких частях реализуемого модуля - 
как для парсинга AOF-репликации, при котором реплика получает команды, закодированные с помощью протокола RESP,
так и для парсинга пользовательских команд создания индекса и поиска по нему.

Поскольку запросы полнотекстового поиска могут выполняться дольше типовых команд, поддерживаемых Redis,
которые успешно обрабатываются его однопоточной моделью выполнения,
необходимо применять конкурентную модель обработки запросов.

## Разработка

### Реализация индексной реплики

Для реализации модуля был выбран язык Go, разработанный компанией Google.
Go является компилируемым языком с автоматическим управлением памятью со сборщиком мусора,
позволяющий легко разрабатывать сложные системы, требующие большой производительности,
в первую очередь благодаря поддержке горутин - легковесных потоков, не привязанных к потокам
операционной системы, что позволяет одновременно работать большому числу таких потоков
(до 1 миллиона и более). Еще одним преимуществом Go является сборка в единый бинарный исполняемый файл,
не требующий виртуальной машины для запуска, что упрощает развертывание компонента.

Применение горутин позволяет упростить создание высокопроизводительного TCP-сервера. 
Так, для обеспечения обработки множества запросов конкурентно, требуется следующий код:

```go
for {
    lnconn, err := s.ln.Accept()
    if err != nil {
        ...
        if errors.Is(err, net.ErrClosed) {
            return nil
        }
        continue
    }
    c := &conn{
        conn: lnconn,
        addr: lnconn.RemoteAddr().String(),
        wr:   NewWriter(lnconn),
        rd:   NewReader(lnconn),
    }
    ...
    go handle(s, c)
}
```

Оператор go вызывает функцию в новой горутине, асинхронно с вызвавшим ее внешним циклом.

Для обработки пользовательских запросов в формате RESP для соответствия протоколу СУБД Redis,
благодаря чему все существующие клиенты не потребуют модификации для взаимодействия с созданным модулем,
была использована библиотека Redcon (github.com/tidwall/redcon). 
Она же была применена для парсинга данных репликации в формате AOF.

Помимо реализации запросов полнотекстового поиска, были также реализованы команды **PPROF START** и **PPROF END**, позволяющие начать профилирование памяти и центрального процессора, 
а затем остановить его и записать статистику в виде файла для проведения последующего анализа.
Это позволяет производить анализ производительности без перезапуска сервера,
что позволило в итоге значительно ее улучшить по результатам анализа.
Также, была реализована вспомогательная команда **COMMAND DOCS**, позволяющая клиенту 
получить информацию о доступных командах, благодаря чему автоматически работают подсказки в
клиенте СУБД Redis для командной строки.

Для чтения реплицированных данных в формате RDB была использована библиотека github.com/hdt3213/rdb,
но для ее применения потребовалось внести в нее изменения для поддержки типов данных, реализованных
встраиваемыми модулями, а не реализованными самой СУБД.

Для хранения в инвертированном индексе сопоставления терминов и содержащих из документов было использовано префиксное дерево, в котором значением является список документов, содержащих этот термин.
Данный список поддерживается в отсортированном виде по возрастанию ключа документа.
Такой подход упростил создание итераторов исполнения поискового запроса, комбинирующих результаты 
(операции пересечения и объединения), поскольку при возврате значений всегда в порядке увеличения ключа
для проверки существования одного ключа в обоих итераторах, данные которых необходимо скомбинировать,
в таком случае не требуется прочитывать полностью - достаточно сравнить текущие значения ключей.
Поддержка массива в отсортированном виде требует дополнительного времени при индексировании документов,
поскольку помимо бинарного поиска позиции нового документа, требуется также копирование данных
в массив большего размера, но необходимое для этого увеличение времени обработки нового документа
допустимо в связи с приоритетом оптимизации времени выполнения поисковых запросов.

Для каждого термина необходимо хранить не только информацию о содержащих его документах,
но и список конкретных вхождений внутри документа, включая позицию термина и поле, которое его содержит.
Для уменьшения размера индекса без вреда и обеспечения большей производительности информация о том,
в каких полях документа присутствует определенный термин, сохраняется отдельно от списка конкретных вхождений
в виде битового массива, что позволяет быструю фильтрацию только тех документов, которые содержат термин
в определенном поле. Информация о самом вхождении также не содержит название поля, а лишь его индекс
в списке индексируемых полей конкретного индекса.

Поскольку используемое префиксное дерево не позволяет конкуретные модификации на запись,
для обеспечения эксклюзивного доступа на запись применяется RWMutex, реализующий
упомянутую ранее блокировку чтения-записи. Для уменьшения времени, на которое RWMutex блокируется на запись,
данные в префиксном дереве обновляются одновременно, 
после индексирования всего документа и расчета метрик TF и DF.

Для того, чтобы корректно выполнять парсинг, было принято решение использовать генератор парсеров ANTLR.
ANTLR позволяет сгенерировать код лексера и парсера на одном из поддерживаемых языков программирования 
по формально описанной грамматике. Благодаря простоте описания грамматики таким образом,
не явилось проблемой обеспечить преобразование запроса с поддержкой нескольких уровней вложенности 
(группировки с помощью скобок) с учетом приоритета операторов. Сгенерированный по описанной грамматике
преобразовывает запрос из текста в набор токенов, а затем также сгенерированный парсер преобразовывает
набор токенов в абстрактное синтаксическое дерево (AST). 
После данных операций необходимо обойти данное дерево, преобразовав обнаруженные в запросе операторы
в комбинации поисковых итераторов. Для выполнения этой задачи был применен распространенный подход,
использующий структуру данных стек для хранения текущего состояния. 
Так, при запросе `hello (spider | man)` парсер сформирует следующее синтаксическое дерево:

![Query AST example](img/query_ast_example.svg)

```plantuml:query_ast_example
object AND
object "word" as hello
hello : text = "hello"
object OR
object "word" as spider
spider : text = "spider"
object "word" as man
man : text = "man"

AND --> hello
AND --> OR
OR --> spider
OR --> man
```

После чего, обход такого дерева должен выполнить следующие шаги:
1. Посетить узел **AND**;
2. Посетить узел **hello**, добавить в стек итератор, читающий вхождения этого термина;
3. Вернуться в узел **AND**;
4. Посетить узел **OR**;
5. Посетить узел **spider**, добавить в стек итератор, читающий вхождения этого термина;
6. Посетить узел **man**, добавить в стек итератор, читающий вхождения этого термина;
7. Вернуться в узел **OR**, прочитать из стека два последних значения (котоорыми будут итераторы для **spider** и **man**), создать объединяющий итератор и добавить его в стек;
8. Вернуться в узел **AND**, прочитать из стека два последних значения (которыми будут итератор для **hello** и объединяющий итератор), создать итератор пересечения и добавить его в стек.

После выполнения этих действий для выполнения запроса необходимо взять единственный итератор, сохраненный в стеке.

### Реализация встраиваемого модуля

СУБД Redis написана на Си, во многом благодаря чему она и обеспечивает хорошую производительность.
Встраиваемый модуль, реализующий новые команды и типы данных, 
должен был собран в виде статической библиотеки, обладающей бинарной совместимостью с Си.
Все функции, доступные модулю, объявлены в файле redismodule.h, 
который необходимо использовать в своей реализации.

Для реализации встраивамого модуля был также использован язык Go, 
поскольку он позволяет обеспечивать совместимость с Си, собирать код в статическую библиотеку,
при этом производительность в результате остается практически на том же уровне, что и
производительность реализаций на Си, сохраняя лучшие простоту и читаемость кода по сравнению с ним.
Еще одним важным преимуществом использования Go для встраиваемого модуля является
переиспользование общего кода, что упрощает совместимость сохранения информации 
о создаваемом индексе на ведущем узле и последующего чтения в индексной реплике.

Код реализации модуля на Go был вынесен в отдельный пакет redismodule, 
что позволит, во-первых, использовать его при необходимости 
для возможной реализации индексирования документов на уровне каждого узла, 
а не только на отдельных индексных репликах, а во-вторых, использовать его в дальнейшем в других проектах.

Для того, чтобы вызвать код на Си из Go, требуется лишь импортировать псевдо-пакет "C" и 
вызвать желаемую функцию, поставив перед ней префикс `C.`. 
Так, вызов функции ReplicateVerbatim из Go осуществляется как C.ReplicateVerbatim.
Для того, чтобы из Си вызывать код на Go, требуется лишь объявить соответствующую функцию в Си с спецификатором extern, 
сообщающим компилятору, что определение функции осуществляется вне текущей единицы трансляции.
